{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasHyun/NTN_implementation/blob/master/ntn_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "L36Z-E-ZXGH8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "\n",
        "# Load data\n",
        "data_path = './drive/MyDrive/data/Wordnet'\n",
        "entities_string = '/entities.txt'\n",
        "relations_string = '/relations.txt'\n",
        "embeds_string = '/initEmbed.mat'\n",
        "training_string = '/train.txt'\n",
        "test_string = '/test.txt'\n",
        "dev_string = '/dev.txt'\n",
        "\n",
        "# 데이터 로드 함수들\n",
        "def load_entities(data_path):\n",
        "    entities_file = open(data_path + '/entities.txt')\n",
        "    entities_list = entities_file.read().strip().split('\\n')\n",
        "    entities_file.close()\n",
        "    return entities_list\n",
        "\n",
        "def load_relations(data_path):\n",
        "    relations_file = open(data_path + '/relations.txt')\n",
        "    relations_list = relations_file.read().strip().split('\\n')\n",
        "    relations_file.close()\n",
        "    return relations_list\n",
        "\n",
        "def load_init_embeds(data_path):\n",
        "    embeds_path = data_path + '/initEmbed.mat'\n",
        "    return load_embeds(embeds_path)\n",
        "\n",
        "def load_embeds(file_path):\n",
        "    mat_contents = sio.loadmat(file_path)\n",
        "    words = mat_contents['words'].squeeze()\n",
        "    we = mat_contents['We']\n",
        "    tree = mat_contents['tree'].squeeze()\n",
        "    word_vecs = [we[:, i].tolist() for i in range(len(words))]\n",
        "    entity_words = [tree[i][0][0][0][0][0].item() for i in range(len(tree))]\n",
        "    return word_vecs, entity_words\n",
        "\n",
        "def load_data(data_path, file_string):\n",
        "    file = open(data_path + file_string)\n",
        "    data = [line.split('\\t') for line in file.read().strip().split('\\n')]\n",
        "    return np.array(data)\n",
        "\n",
        "def data_to_indexed(data, entities, relations):\n",
        "    entity_to_index = {entities[i]: i for i in range(len(entities))}\n",
        "    relation_to_index = {relations[i]: i for i in range(len(relations))}\n",
        "    indexed_data = [(entity_to_index[data[i][0]], relation_to_index[data[i][1]], entity_to_index[data[i][2]]) for i in range(len(data))]\n",
        "    return indexed_data\n",
        "\n",
        "def get_batch(batch_size, data, num_entities, corrupt_size):\n",
        "    random_indices = random.sample(range(len(data)), batch_size)\n",
        "    batch = [(data[i][0], data[i][1], data[i][2], random.randint(0, num_entities - 1)) for i in random_indices for _ in range(corrupt_size)]\n",
        "    return batch\n",
        "\n",
        "def split_batch(data_batch, num_relations):\n",
        "    batches = [[] for _ in range(num_relations)]\n",
        "    for e1, r, e2, e3 in data_batch:\n",
        "        batches[r].append((e1, e2, e3))\n",
        "    return batches\n",
        "\n",
        "def fill_feed_dict(batches, train_both, batch_placeholders, label_placeholders, corrupt_placeholder):\n",
        "    feed_dict = {corrupt_placeholder: [train_both and np.random.random() > 0.5]}\n",
        "    for i in range(len(batch_placeholders)):\n",
        "        feed_dict[batch_placeholders[i]] = batches[i]\n",
        "        feed_dict[label_placeholders[i]] = [[0.0] for _ in range(len(batches[i]))]\n",
        "    return feed_dict\n",
        "\n",
        "# 모델 클래스 정의\n",
        "class CustomModel(tf.keras.Model):\n",
        "    def __init__(self, init_word_embeds, entity_to_wordvec, num_entities, num_relations, slice_size, batch_size):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.init_word_embeds = init_word_embeds\n",
        "        self.entity_to_wordvec = entity_to_wordvec\n",
        "        self.num_entities = num_entities\n",
        "        self.num_relations = num_relations\n",
        "        self.slice_size = slice_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        d = 100\n",
        "        k = self.slice_size\n",
        "        num_words = len(self.init_word_embeds)\n",
        "        self.E = tf.Variable(self.init_word_embeds, trainable=True, dtype=tf.float32)\n",
        "        self.W = [self.add_weight(shape=(d, d, k), initializer='random_normal', trainable=True, dtype=tf.float32) for _ in range(self.num_relations)]\n",
        "        self.V = [self.add_weight(shape=(k, 2 * d), initializer='zeros', trainable=True, dtype=tf.float32) for _ in range(self.num_relations)]\n",
        "        print(\"self.V[r] shape:\", self.V[0].shape)\n",
        "        self.b = [self.add_weight(shape=(k, 1), initializer='zeros', trainable=True, dtype=tf.float32) for _ in range(self.num_relations)]\n",
        "        self.U = [self.add_weight(shape=(1, k), initializer='ones', trainable=True, dtype=tf.float32, name=f'U_{r}') for r in range(self.num_relations)]\n",
        "        super(CustomModel, self).build(input_shape)\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs, training=None):\n",
        "        e1, e2, e3 = tf.cast(inputs[..., 0], tf.int32), tf.cast(inputs[..., 1], tf.int32), tf.cast(inputs[..., 2], tf.int32)\n",
        "        ent2word = [tf.constant([entity_i], dtype=tf.int32) - 1 for entity_i in self.entity_to_wordvec]\n",
        "        entEmbed = tf.stack([tf.reduce_mean(tf.gather(self.E, entword), axis=0) for entword in ent2word])\n",
        "\n",
        "        predictions = []\n",
        "        for r in range(self.num_relations):\n",
        "            e1v = tf.transpose(tf.gather(entEmbed, e1))\n",
        "            e2v = tf.transpose(tf.gather(entEmbed, e2))\n",
        "            e3v = tf.transpose(tf.gather(entEmbed, e3))\n",
        "\n",
        "            if len(e1v.shape) > 1 and e1v.shape[0] == 1:\n",
        "                e1v = tf.squeeze(e1v, axis=0)\n",
        "            if len(e2v.shape) > 1 and e2v.shape[0] == 1:\n",
        "                e2v = tf.squeeze(e2v, axis=0)\n",
        "            if len(e3v.shape) > 1 and e3v.shape[0] == 1:\n",
        "                e3v = tf.squeeze(e3v, axis=0)\n",
        "\n",
        "\n",
        "            print(\"e1v shape:\", e1v.shape)\n",
        "            print(\"e2v shape:\", e2v.shape)\n",
        "            print(\"e3v shape:\", e3v.shape)\n",
        "            e1v_pos = e1v\n",
        "            e2v_pos = e2v\n",
        "            e1v_neg = e1v\n",
        "            e2v_neg = e3v\n",
        "            num_rel_r = tf.expand_dims(tf.shape(e1v_pos)[0], 0)\n",
        "            preactivation_pos = []\n",
        "            preactivation_neg = []\n",
        "\n",
        "            for slice in range(self.slice_size):\n",
        "                preactivation_pos.append(tf.reduce_sum(e1v_pos * tf.matmul(self.W[r][:, :, slice], e2v_pos), axis=1))\n",
        "                preactivation_neg.append(tf.reduce_sum(e1v_neg * tf.matmul(self.W[r][:, :, slice], e2v_neg), axis=1))\n",
        "\n",
        "            preactivation_pos = tf.stack(preactivation_pos, axis=1)\n",
        "            preactivation_neg = tf.stack(preactivation_neg, axis=1)\n",
        "\n",
        "            temp2_pos = tf.matmul(self.V[r], tf.concat([e1v_pos, e2v_pos], axis=0))\n",
        "            temp2_neg = tf.matmul(self.V[r], tf.concat([e1v_neg, e2v_neg], axis=0))\n",
        "\n",
        "            # Match shapes by expanding the bias\n",
        "            bias_r = tf.broadcast_to(tf.squeeze(self.b[r], axis=-1), preactivation_pos.shape)\n",
        "\n",
        "            preactivation_pos = preactivation_pos + temp2_pos + bias_r\n",
        "            preactivation_neg = preactivation_neg + temp2_neg + bias_r\n",
        "\n",
        "            activation_pos = tf.math.tanh(preactivation_pos)\n",
        "            activation_neg = tf.math.tanh(preactivation_neg)\n",
        "\n",
        "            score_pos = tf.reshape(tf.matmul(self.U[r], activation_pos), num_rel_r)\n",
        "            score_neg = tf.reshape(tf.matmul(self.U[r], activation_neg), num_rel_r)\n",
        "\n",
        "            predictions.append(tf.stack([score_pos, score_neg], axis=1))\n",
        "\n",
        "        predictions = tf.concat(predictions, axis=1)\n",
        "        return predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tf.TensorShape([None, self.num_relations, 2])\n",
        "\n",
        "\n",
        "# 손실 함수 정의\n",
        "def loss_fn(predictions, regularization):\n",
        "    temp1 = tf.maximum(tf.subtract(predictions[:, 1], predictions[:, 0]) + 1, 0)\n",
        "    temp1 = tf.reduce_sum(temp1)\n",
        "    temp2 = tf.sqrt(sum([tf.reduce_sum(tf.square(var)) for var in tf.compat.v1.trainable_variables()]))\n",
        "    temp = temp1 + (regularization * temp2)\n",
        "    return temp\n",
        "\n",
        "# 훈련 스텝 함수 정의\n",
        "@tf.function\n",
        "def train_step(model, optimizer, loss_fn, data, batch_size, num_entities, corrupt_size, num_relations, train_both):\n",
        "    with tf.GradientTape() as tape:\n",
        "        data_batch = get_batch(batch_size, data, num_entities, corrupt_size)\n",
        "        relation_batches = split_batch(data_batch, num_relations)\n",
        "        inputs = tf.constant(np.vstack(relation_batches))\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss_value = loss_fn(predictions, 0.0001)\n",
        "\n",
        "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss_value\n",
        "\n",
        "@tf.function\n",
        "def evaluate(model, data, batch_size, num_entities, num_relations):\n",
        "    hits = 0\n",
        "    total = 0\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch_data = data[i:i + batch_size]\n",
        "        data_batch = get_batch(len(batch_data), batch_data, num_entities, 1)\n",
        "        relation_batches = split_batch(data_batch, num_relations)\n",
        "        inputs = tf.constant(np.vstack(relation_batches))\n",
        "        preds = model(inputs, training=False)\n",
        "\n",
        "        for r in range(num_relations):\n",
        "            for j in range(len(relation_batches[r])):\n",
        "                if preds[r][j][0] > 0.5:  # Thresholding at 0.\n",
        "                    hits += 1\n",
        "                total += 1\n",
        "\n",
        "    return hits / total\n",
        "\n",
        "\n",
        "\n",
        "def train_epoch(model, optimizer, loss_fn, data, batch_size, num_entities, corrupt_size, num_relations, train_both):\n",
        "    total_loss = 0.0\n",
        "    num_batches = len(data) // batch_size\n",
        "    for _ in range(num_batches):\n",
        "        loss_value = train_step(model, optimizer, loss_fn, data, batch_size, num_entities, corrupt_size, num_relations, train_both)\n",
        "        total_loss += loss_value.numpy()\n",
        "    return total_loss / num_batches\n",
        "\n",
        "# 전체 훈련 및 평가 함수\n",
        "# def run_training():\n",
        "#     print(\"Begin!\")\n",
        "#     print(\"Load training data...\")\n",
        "#     raw_training_data = load_data(data_path, training_string)\n",
        "#     print(\"Load validation data...\")\n",
        "#     raw_dev_data = load_data(data_path, dev_string)\n",
        "#     print(\"Load test data...\")\n",
        "#     raw_test_data = load_data(data_path, test_string)\n",
        "\n",
        "#     print(\"Load entities and relations...\")\n",
        "#     entities_list = load_entities(data_path)\n",
        "#     relations_list = load_relations(data_path)\n",
        "#     indexed_training_data = data_to_indexed(raw_training_data, entities_list, relations_list)\n",
        "#     indexed_dev_data = data_to_indexed(raw_dev_data, entities_list, relations_list)\n",
        "#     indexed_test_data = data_to_indexed(raw_test_data, entities_list, relations_list)\n",
        "\n",
        "#     print(\"Load embeddings...\")\n",
        "#     init_word_embeds, entity_to_wordvec = load_init_embeds(data_path)\n",
        "\n",
        "#     num_entities = len(entities_list)\n",
        "#     num_relations = len(relations_list)\n",
        "#     num_iters = 500\n",
        "#     batch_size = 20000\n",
        "#     corrupt_size = 10\n",
        "#     slice_size = 3\n",
        "#     regularization = 0.0001\n",
        "#     learning_rate = 0.01\n",
        "#     save_per_iter = 10\n",
        "#     train_both = False\n",
        "\n",
        "#     print(f\"Starting to build model {datetime.datetime.now()}\")\n",
        "\n",
        "#     model = CustomModel(init_word_embeds, entity_to_wordvec, num_entities, num_relations, slice_size, batch_size)\n",
        "\n",
        "#     optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "#     for epoch in range(1, num_iters + 1):\n",
        "#         print(f\"Starting epoch {epoch} {datetime.datetime.now()}\")\n",
        "#         train_loss = train_epoch(model, optimizer, loss_fn, indexed_training_data, batch_size, num_entities, corrupt_size, num_relations, train_both)\n",
        "#         print(f\"Epoch {epoch} loss: {train_loss}\")\n",
        "\n",
        "#         if epoch % save_per_iter == 0:\n",
        "#             model.save(f\"{data_path}/model_epoch_{epoch}\")\n",
        "\n",
        "#         if epoch % save_per_iter == 0:\n",
        "#             print(f\"Evaluating on dev data at epoch {epoch}\")\n",
        "#             dev_accuracy = evaluate(model, indexed_dev_data, batch_size, num_entities, num_relations)\n",
        "#             print(f\"Dev accuracy at epoch {epoch}: {dev_accuracy}\")\n",
        "\n",
        "#     print(\"Training finished!\")\n",
        "#     print(\"Evaluating on test data\")\n",
        "#     test_accuracy = evaluate(model, indexed_test_data, batch_size, num_entities, num_relations)\n",
        "#     print(f\"Test accuracy: {test_accuracy}\")\n",
        "\n",
        "# run_training()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyeQAl0edJhQ",
        "outputId": "00c1a2b1-0363-41e3-aef6-1fbb10f379b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Begin!\n",
            "Load training data...\n",
            "Load validation data...\n",
            "Load test data...\n"
          ]
        }
      ],
      "source": [
        "print(\"Begin!\")\n",
        "print(\"Load training data...\")\n",
        "raw_training_data = load_data(data_path, training_string)\n",
        "print(\"Load validation data...\")\n",
        "raw_dev_data = load_data(data_path, dev_string)\n",
        "print(\"Load test data...\")\n",
        "raw_test_data = load_data(data_path, test_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5GfFbT9__5t",
        "outputId": "8b75ccfc-7613-4d2a-d129-eda2112aa346"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load entities and relations...\n",
            "Load embeddings...\n"
          ]
        }
      ],
      "source": [
        "print(\"Load entities and relations...\")\n",
        "entities_list = load_entities(data_path)\n",
        "relations_list = load_relations(data_path)\n",
        "indexed_training_data = data_to_indexed(raw_training_data, entities_list, relations_list)\n",
        "indexed_dev_data = data_to_indexed(raw_dev_data, entities_list, relations_list)\n",
        "indexed_test_data = data_to_indexed(raw_test_data, entities_list, relations_list)\n",
        "\n",
        "print(\"Load embeddings...\")\n",
        "init_word_embeds, entity_to_wordvec = load_init_embeds(data_path)\n",
        "\n",
        "num_entities = len(entities_list)\n",
        "num_relations = len(relations_list)\n",
        "num_iters = 500\n",
        "batch_size = 20000\n",
        "corrupt_size = 10\n",
        "slice_size = 3\n",
        "regularization = 0.0001\n",
        "learning_rate = 0.01\n",
        "save_per_iter = 10\n",
        "train_both = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLzBVnwuADiH",
        "outputId": "2195b9fa-9ff6-4a1a-f107-d171157ca846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting to build model 2024-05-18 18:56:53.503069\n"
          ]
        }
      ],
      "source": [
        "print(f\"Starting to build model {datetime.datetime.now()}\")\n",
        "\n",
        "model = CustomModel(init_word_embeds, entity_to_wordvec, num_entities, num_relations, slice_size, batch_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNl3MGI1AXXM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5daf02-1b80-4cb7-980a-f1830a6e5be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1 2024-05-18 19:07:51.348969\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, num_iters + 1):\n",
        "    print(f\"Starting epoch {epoch} {datetime.datetime.now()}\")\n",
        "    train_loss = train_epoch(model, optimizer, loss_fn, indexed_training_data, batch_size, num_entities, corrupt_size, num_relations, train_both)\n",
        "    print(f\"Epoch {epoch} loss: {train_loss}\")\n",
        "\n",
        "    if epoch % save_per_iter == 0:\n",
        "        model.save(f\"{data_path}/model_epoch_{epoch}\")\n",
        "\n",
        "    if epoch % save_per_iter == 0:\n",
        "        print(f\"Evaluating on dev data at epoch {epoch}\")\n",
        "        dev_accuracy = evaluate(model, indexed_dev_data, batch_size, num_entities, num_relations)\n",
        "        print(f\"Dev accuracy at epoch {epoch}: {dev_accuracy}\")\n",
        "\n",
        "print(\"Training finished!\")\n",
        "print(\"Evaluating on test data\")\n",
        "test_accuracy = evaluate(model, indexed_test_data, batch_size, num_entities, num_relations)\n",
        "print(f\"Test accuracy: {test_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1fR01q5BmAl0bKmzjKfM-f1AH96C4FCkL",
      "authorship_tag": "ABX9TyOdbzIBNGP+Ei3PaPcscqbG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
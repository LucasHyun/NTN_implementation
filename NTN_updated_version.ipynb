{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "1QjSDV_fLYleBsxtd0F90YzYG3GF9us09",
      "authorship_tag": "ABX9TyM2IVt1S/Moh9Z+FyVbM+8T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasHyun/NTN_implementation/blob/master/NTN_updated_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiGj5VhqMSva",
        "outputId": "4244d469-2be6-4cfb-b747-6b8bff655761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin!\n",
            "Load training data...\n",
            "Load validation data...\n",
            "Load test data...\n",
            "Load entities and relations...\n",
            "Load embeddings...\n",
            "Starting to build model 2024-05-19 06:23:53.730515\n",
            "Starting epoch 1 2024-05-19 06:24:35.786323\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "final predictions shape: (1, 22, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "score_pos shape: (1, 640)\n",
            "score_neg shape: (1, 640)\n",
            "final predictions shape: (1, 22, 640)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "\n",
        "# Load data\n",
        "data_path = './drive/MyDrive/data/Wordnet'\n",
        "entities_string = '/entities.txt'\n",
        "relations_string = '/relations.txt'\n",
        "embeds_string = '/initEmbed.mat'\n",
        "training_string = '/train.txt'\n",
        "test_string = '/test.txt'\n",
        "dev_string = '/dev.txt'\n",
        "\n",
        "# 데이터 로드 함수들\n",
        "def load_entities(data_path):\n",
        "    entities_file = open(data_path + '/entities.txt')\n",
        "    entities_list = entities_file.read().strip().split('\\n')\n",
        "    entities_file.close()\n",
        "    return entities_list\n",
        "\n",
        "def load_relations(data_path):\n",
        "    relations_file = open(data_path + '/relations.txt')\n",
        "    relations_list = relations_file.read().strip().split('\\n')\n",
        "    relations_file.close()\n",
        "    return relations_list\n",
        "\n",
        "def load_init_embeds(data_path):\n",
        "    embeds_path = data_path + '/initEmbed.mat'\n",
        "    return load_embeds(embeds_path)\n",
        "\n",
        "def load_embeds(file_path):\n",
        "    mat_contents = sio.loadmat(file_path)\n",
        "    words = mat_contents['words'].squeeze()\n",
        "    we = mat_contents['We']\n",
        "    tree = mat_contents['tree'].squeeze()\n",
        "    word_vecs = [we[:, i].tolist() for i in range(len(words))]\n",
        "    entity_words = [tree[i][0][0][0][0][0].item() for i in range(len(tree))]\n",
        "    return word_vecs, entity_words\n",
        "\n",
        "def load_data(data_path, file_string):\n",
        "    file = open(data_path + file_string)\n",
        "    data = [line.split('\\t') for line in file.read().strip().split('\\n')]\n",
        "    return np.array(data)\n",
        "\n",
        "def data_to_indexed(data, entities, relations):\n",
        "    entity_to_index = {entities[i]: i for i in range(len(entities))}\n",
        "    relation_to_index = {relations[i]: i for i in range(len(relations))}\n",
        "    indexed_data = [(entity_to_index[data[i][0]], relation_to_index[data[i][1]], entity_to_index[data[i][2]]) for i in range(len(data))]\n",
        "    return indexed_data\n",
        "\n",
        "def get_batch(batch_size, data, num_entities, corrupt_size):\n",
        "    random_indices = random.sample(range(len(data)), batch_size)\n",
        "    batch = [(data[i][0], data[i][1], data[i][2], random.randint(0, num_entities - 1)) for i in random_indices for _ in range(corrupt_size)]\n",
        "    return batch\n",
        "\n",
        "# def split_batch(data_batch, num_relations):\n",
        "#     batches = [[] for _ in range(num_relations)]\n",
        "#     for e1, r, e2, e3 in data_batch:\n",
        "#         batches[r].append((e1, e2, e3))\n",
        "#     return batches\n",
        "def split_batch(data_batch, num_relations):\n",
        "    batches = [[] for _ in range(num_relations)]\n",
        "    for e1, r, e2, e3 in data_batch:\n",
        "        batches[r].append((e1, e2, e3))\n",
        "    return [batch for batch in batches if batch]\n",
        "\n",
        "def fill_feed_dict(batches, train_both, batch_placeholders, label_placeholders, corrupt_placeholder):\n",
        "    feed_dict = {corrupt_placeholder: [train_both and np.random.random() > 0.5]}\n",
        "    for i in range(len(batch_placeholders)):\n",
        "        feed_dict[batch_placeholders[i]] = batches[i]\n",
        "        feed_dict[label_placeholders[i]] = [[0.0] for _ in range(len(batches[i]))]\n",
        "    return feed_dict\n",
        "\n",
        "# 모델 클래스 정의\n",
        "class CustomModel(tf.keras.Model):\n",
        "    def __init__(self, init_word_embeds, entity_to_wordvec, num_entities, num_relations, slice_size, batch_size):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.init_word_embeds = init_word_embeds\n",
        "        self.entity_to_wordvec = entity_to_wordvec\n",
        "        self.num_entities = num_entities\n",
        "        self.num_relations = num_relations\n",
        "        self.slice_size = slice_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        d = 100\n",
        "        k = self.slice_size #slice size\n",
        "        num_words = len(self.init_word_embeds)\n",
        "        self.E = tf.Variable(self.init_word_embeds, trainable=True, dtype=tf.float32)\n",
        "        self.W = [self.add_weight(shape=(d, d, k), initializer='random_normal', trainable=True, dtype=tf.float32) for _ in range(self.num_relations)]\n",
        "        self.V = [self.add_weight(shape=(k, 2 * d), initializer='zeros', trainable=True, dtype=tf.float32) for _ in range(self.num_relations)]\n",
        "        # print(\"self.V[r] shape:\", self.V[0].shape)\n",
        "        self.b = [self.add_weight(shape=(k, 1), initializer='zeros', trainable=True, dtype=tf.float32) for _ in range(self.num_relations)]\n",
        "        self.U = [self.add_weight(shape=(1, k), initializer='ones', trainable=True, dtype=tf.float32, name=f'U_{r}') for r in range(self.num_relations)]\n",
        "        super(CustomModel, self).build(input_shape)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs, training=None):\n",
        "        # Extracting indices\n",
        "        e1, e2, e3 = tf.cast(inputs[..., 0], tf.int32), tf.cast(inputs[..., 1], tf.int32), tf.cast(inputs[..., 2], tf.int32)\n",
        "        # print(\"e1 shape:\", e1.shape)  # (batch_size,)\n",
        "        # print(\"e2 shape:\", e2.shape)  # (batch_size,)\n",
        "        # print(\"e3 shape:\", e3.shape)  # (batch_size,)\n",
        "\n",
        "        # Mapping entities to word vectors and averaging\n",
        "        ent2word = [tf.constant([entity_i], dtype=tf.int32) - 1 for entity_i in self.entity_to_wordvec]\n",
        "        entEmbed = tf.stack([tf.reduce_mean(tf.gather(self.E, entword), axis=0) for entword in ent2word])\n",
        "        # print(\"entEmbed shape:\", entEmbed.shape)  # (num_entities, d)\n",
        "\n",
        "        predictions = []\n",
        "        for r in range(self.num_relations):\n",
        "            # Gathering embeddings\n",
        "            e1v = tf.transpose(tf.gather(entEmbed, e1))  # (d, batch_size)\n",
        "            e2v = tf.transpose(tf.gather(entEmbed, e2))  # (d, batch_size)\n",
        "            e3v = tf.transpose(tf.gather(entEmbed, e3))  # (d, batch_size)\n",
        "            # print(\"e1v shape before squeeze:\", e1v.shape)\n",
        "            # print(\"e2v shape before squeeze:\", e2v.shape)\n",
        "            # print(\"e3v shape before squeeze:\", e3v.shape)\n",
        "\n",
        "            if len(e1v.shape) > 1 and e1v.shape[0] == 1:\n",
        "                e1v = tf.squeeze(e1v, axis=0)\n",
        "            if len(e2v.shape) > 1 and e2v.shape[0] == 1:\n",
        "                e2v = tf.squeeze(e2v, axis=0)\n",
        "            if len(e3v.shape) > 1 and e3v.shape[0] == 1:\n",
        "                e3v = tf.squeeze(e3v, axis=0)\n",
        "            # print(\"e1v shape after squeeze:\", e1v.shape)\n",
        "            # print(\"e2v shape after squeeze:\", e2v.shape)\n",
        "            # print(\"e3v shape after squeeze:\", e3v.shape)\n",
        "\n",
        "            e1v_pos = e1v\n",
        "            e2v_pos = e2v\n",
        "            e1v_neg = e1v\n",
        "            e2v_neg = e3v\n",
        "\n",
        "            num_rel_r = tf.expand_dims(tf.shape(e1v_pos)[0], 0)\n",
        "\n",
        "            preactivation_pos = []\n",
        "            preactivation_neg = []\n",
        "            for slice in range(self.slice_size):\n",
        "              preactivation_pos.append(tf.reduce_sum(e1v * tf.matmul(self.W[r][:, :, slice], e2v), axis=0))\n",
        "              preactivation_neg.append(tf.reduce_sum(e1v * tf.matmul(self.W[r][:, :, slice], e3v), axis=0))\n",
        "            preactivation_pos = tf.stack(preactivation_pos, axis=0)  # Shape: (slice_size, batch_size)\n",
        "            preactivation_neg = tf.stack(preactivation_neg, axis=0)\n",
        "\n",
        "            # print(\"preactivation_pos shape:\", preactivation_pos.shape)  # (slice_size, batch_size)\n",
        "            # print(\"preactivation_neg shape:\", preactivation_neg.shape)  # (slice_size, batch_size)\n",
        "\n",
        "            concat_pos = tf.concat([e1v, e2v], axis=0)  # (2*d, batch_size)\n",
        "            concat_neg = tf.concat([e1v, e3v], axis=0)  # (2*d, batch_size)\n",
        "            # print(\"concat_pos shape:\", concat_pos.shape)  # (2 * d, batch_size)\n",
        "            # print(\"concat_neg shape:\", concat_neg.shape)  # (2 * d, batch_size)\n",
        "\n",
        "            temp2_pos = tf.matmul(self.V[r], concat_pos)  # (slice_size, batch_size)\n",
        "            temp2_neg = tf.matmul(self.V[r], concat_neg)\n",
        "            # print(\"temp2_pos shape:\", temp2_pos.shape)  # (slice_size, batch_size)\n",
        "            # print(\"temp2_neg shape:\", temp2_neg.shape)  # (slice_size, batch_size)\n",
        "\n",
        "            bias_r = tf.squeeze(self.b[r], axis=-1)[:, None]\n",
        "\n",
        "            preactivation_pos = preactivation_pos + temp2_pos + bias_r\n",
        "            preactivation_neg = preactivation_neg + temp2_neg + bias_r\n",
        "            # print(\"preactivation_pos shape after addition:\", preactivation_pos.shape)  # (slice_size, batch_size)\n",
        "            # print(\"preactivation_neg shape after addition:\", preactivation_neg.shape)  # (slice_size, batch_size)\n",
        "\n",
        "            activation_pos = tf.math.tanh(preactivation_pos)\n",
        "            activation_neg = tf.math.tanh(preactivation_neg)\n",
        "            # print(\"activation_pos shape:\", activation_pos.shape)  # (slice_size, batch_size)\n",
        "            # print(\"activation_neg shape:\", activation_neg.shape)  # (slice_size, batch_size)\n",
        "\n",
        "            # score_pos = tf.reshape(tf.matmul(self.U[r], activation_pos, transpose_b=True), num_rel_r)\n",
        "            # score_neg = tf.reshape(tf.matmul(self.U[r], activation_neg, transpose_b=True), num_rel_r)\n",
        "            # score_pos = tf.reshape(tf.matmul(tf.transpose(self.U[r]), activation_pos), num_rel_r)\n",
        "            # score_neg = tf.reshape(tf.matmul(tf.transpose(self.U[r]), activation_neg), num_rel_r)\n",
        "            score_pos = tf.matmul(self.U[r], activation_pos)\n",
        "            score_neg = tf.matmul(self.U[r], activation_neg)\n",
        "            # print(\"score_pos shape:\", score_pos.shape)  # (1, batch_size)\n",
        "            # print(\"score_neg shape:\", score_neg.shape)  # (1, batch_size)\n",
        "\n",
        "            predictions.append(tf.stack([score_pos, score_neg], axis=1))\n",
        "\n",
        "        predictions = tf.concat(predictions, axis=1)\n",
        "        print(\"final predictions shape:\", predictions.shape)\n",
        "        return predictions\n",
        "\n",
        "    # @tf.function\n",
        "    # def call(self, inputs, training=None):\n",
        "    #     e1, e2, e3 = tf.cast(inputs[..., 0], tf.int32), tf.cast(inputs[..., 1], tf.int32), tf.cast(inputs[..., 2], tf.int32)\n",
        "    #     ent2word = [tf.constant([entity_i], dtype=tf.int32) - 1 for entity_i in self.entity_to_wordvec]\n",
        "    #     entEmbed = tf.stack([tf.reduce_mean(tf.gather(self.E, entword), axis=0) for entword in ent2word])\n",
        "\n",
        "    #     predictions = []\n",
        "    #     for r in range(self.num_relations):\n",
        "    #         e1v = tf.transpose(tf.gather(entEmbed, e1))\n",
        "    #         e2v = tf.transpose(tf.gather(entEmbed, e2))\n",
        "    #         e3v = tf.transpose(tf.gather(entEmbed, e3))\n",
        "\n",
        "    #         if len(e1v.shape) > 1 and e1v.shape[0] == 1:\n",
        "    #             e1v = tf.squeeze(e1v, axis=0)\n",
        "    #         if len(e2v.shape) > 1 and e2v.shape[0] == 1:\n",
        "    #             e2v = tf.squeeze(e2v, axis=0)\n",
        "    #         if len(e3v.shape) > 1 and e3v.shape[0] == 1:\n",
        "    #             e3v = tf.squeeze(e3v, axis=0)\n",
        "\n",
        "\n",
        "    #         print(\"e1v shape:\", e1v.shape)\n",
        "    #         print(\"e2v shape:\", e2v.shape)\n",
        "    #         print(\"e3v shape:\", e3v.shape)\n",
        "    #         e1v_pos = e1v\n",
        "    #         e2v_pos = e2v\n",
        "    #         e1v_neg = e1v\n",
        "    #         e2v_neg = e3v\n",
        "    #         num_rel_r = tf.expand_dims(tf.shape(e1v_pos)[0], 0)\n",
        "    #         preactivation_pos = []\n",
        "    #         preactivation_neg = []\n",
        "\n",
        "    #         for slice in range(self.slice_size):\n",
        "    #             preactivation_pos.append(tf.reduce_sum(e1v_pos * tf.matmul(self.W[r][:, :, slice], e2v_pos), axis=1))\n",
        "    #             preactivation_neg.append(tf.reduce_sum(e1v_neg * tf.matmul(self.W[r][:, :, slice], e2v_neg), axis=1))\n",
        "\n",
        "    #         preactivation_pos = tf.stack(preactivation_pos, axis=1) #Shape: ()\n",
        "    #         preactivation_neg = tf.stack(preactivation_neg, axis=1)\n",
        "    #         print(preactivation_pos.shape)\n",
        "    #         print(preactivation_neg.shape)\n",
        "    #         concat_pos = tf.concat([e1v_pos, e2v_pos], axis=0)  # Shape: (200, 200000)\n",
        "    #         concat_neg = tf.concat([e1v_neg, e2v_neg], axis=0)  # Shape: (200, 200000)\n",
        "    #         temp2_pos = tf.transpose(tf.matmul(self.V[r], concat_pos)) # 200000 * 3\n",
        "    #         temp2_neg = tf.transpose(tf.matmul(self.V[r], concat_neg))\n",
        "    #         # temp2_pos = tf.matmul(tf.concat([e1v_pos, e2v_pos], axis=1), tf.transpose(self.V[r]))\n",
        "    #         # temp2_neg = tf.matmul(tf.concat([e1v_neg, e2v_neg], axis=1), tf.transpose(self.V[r]))\n",
        "    #         # Match shapes by expanding the bias\n",
        "    #         bias_r = tf.broadcast_to(tf.squeeze(self.b[r], axis=-1), preactivation_pos.shape)\n",
        "    #         print(bias_r.shape)\n",
        "    #         preactivation_pos = preactivation_pos + temp2_pos + bias_r\n",
        "    #         preactivation_neg = preactivation_neg + temp2_neg + bias_r\n",
        "\n",
        "    #         activation_pos = tf.math.tanh(preactivation_pos)\n",
        "    #         activation_neg = tf.math.tanh(preactivation_neg)\n",
        "\n",
        "    #         score_pos = tf.reshape(tf.matmul(self.U[r], activation_pos), num_rel_r)\n",
        "    #         score_neg = tf.reshape(tf.matmul(self.U[r], activation_neg), num_rel_r)\n",
        "\n",
        "    #         predictions.append(tf.stack([score_pos, score_neg], axis=1))\n",
        "\n",
        "    #     predictions = tf.concat(predictions, axis=1)\n",
        "    #     return predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tf.TensorShape([None, self.num_relations, 2])\n",
        "\n",
        "\n",
        "# 손실 함수 정의\n",
        "# def loss_fn(predictions, regularization):\n",
        "#     temp1 = tf.maximum(tf.subtract(predictions[:, 1], predictions[:, 0]) + 1, 0)\n",
        "#     temp1 = tf.reduce_sum(temp1)\n",
        "#     temp2 = tf.sqrt(sum([tf.reduce_sum(tf.square(var)) for var in tf.compat.v1.trainable_variables()]))\n",
        "#     temp = temp1 + (regularization * temp2)\n",
        "#     return temp\n",
        "# 손실 함수 정의\n",
        "def loss_fn(predictions, regularization):\n",
        "    temp1 = tf.maximum(tf.subtract(predictions[:, 1], predictions[:, 0]) + 1, 0)\n",
        "    temp1 = tf.reduce_sum(temp1)\n",
        "    temp2 = tf.sqrt(tf.cast(sum([tf.reduce_sum(tf.square(tf.cast(var, tf.float32))) for var in tf.compat.v1.trainable_variables()]), tf.float32))\n",
        "    temp = temp1 + (regularization * temp2)\n",
        "    return temp\n",
        "\n",
        "\n",
        "# 훈련 스텝 함수 정의\n",
        "@tf.function\n",
        "def train_step(model, optimizer, loss_fn, data, batch_size, num_entities, corrupt_size, num_relations, train_both):\n",
        "    with tf.GradientTape() as tape:\n",
        "        data_batch = get_batch(batch_size, data, num_entities, corrupt_size)\n",
        "        relation_batches = split_batch(data_batch, num_relations)\n",
        "        inputs = tf.constant(np.vstack(relation_batches))\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss_value = loss_fn(predictions, 0.0001)\n",
        "\n",
        "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss_value\n",
        "\n",
        "@tf.function\n",
        "def evaluate(model, data, batch_size, num_entities, num_relations):\n",
        "    hits = 0\n",
        "    total = 0\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch_data = data[i:i + batch_size]\n",
        "        data_batch = get_batch(len(batch_data), batch_data, num_entities, 1)\n",
        "        relation_batches = split_batch(data_batch, num_relations)\n",
        "        inputs = tf.constant(np.vstack(relation_batches))\n",
        "        preds = model(inputs, training=False)\n",
        "\n",
        "        for r in range(num_relations):\n",
        "            for j in range(len(relation_batches[r])):\n",
        "                if preds[r][j][0] > 0.5:  # Thresholding at 0.\n",
        "                    hits += 1\n",
        "                total += 1\n",
        "\n",
        "    return hits / total\n",
        "\n",
        "\n",
        "\n",
        "def train_epoch(model, optimizer, loss_fn, data, batch_size, num_entities, corrupt_size, num_relations, train_both):\n",
        "    total_loss = 0.0\n",
        "    num_batches = len(data) // batch_size\n",
        "    for _ in range(num_batches):\n",
        "        loss_value = train_step(model, optimizer, loss_fn, data, batch_size, num_entities, corrupt_size, num_relations, train_both)\n",
        "        total_loss += loss_value.numpy()\n",
        "    return total_loss / num_batches\n",
        "\n",
        "# 전체 훈련 및 평가 함수\n",
        "# def run_training():\n",
        "#     print(\"Begin!\")\n",
        "#     print(\"Load training data...\")\n",
        "#     raw_training_data = load_data(data_path, training_string)\n",
        "#     print(\"Load validation data...\")\n",
        "#     raw_dev_data = load_data(data_path, dev_string)\n",
        "#     print(\"Load test data...\")\n",
        "#     raw_test_data = load_data(data_path, test_string)\n",
        "\n",
        "#     print(\"Load entities and relations...\")\n",
        "#     entities_list = load_entities(data_path)\n",
        "#     relations_list = load_relations(data_path)\n",
        "#     indexed_training_data = data_to_indexed(raw_training_data, entities_list, relations_list)\n",
        "#     indexed_dev_data = data_to_indexed(raw_dev_data, entities_list, relations_list)\n",
        "#     indexed_test_data = data_to_indexed(raw_test_data, entities_list, relations_list)\n",
        "\n",
        "#     print(\"Load embeddings...\")\n",
        "#     init_word_embeds, entity_to_wordvec = load_init_embeds(data_path)\n",
        "\n",
        "#     num_entities = len(entities_list)\n",
        "#     num_relations = len(relations_list)\n",
        "#     num_iters = 500\n",
        "#     batch_size = 100\n",
        "#     corrupt_size = 10\n",
        "#     slice_size = 3\n",
        "#     regularization = 0.0001\n",
        "#     learning_rate = 0.01\n",
        "#     save_per_iter = 10\n",
        "#     train_both = False\n",
        "\n",
        "#     print(f\"Starting to build model {datetime.datetime.now()}\")\n",
        "\n",
        "#     model = CustomModel(init_word_embeds, entity_to_wordvec, num_entities, num_relations, slice_size, batch_size)\n",
        "\n",
        "#     optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "#     for epoch in range(1, num_iters + 1):\n",
        "#         print(f\"Starting epoch {epoch} {datetime.datetime.now()}\")\n",
        "#         train_loss = train_epoch(model, optimizer, loss_fn, indexed_training_data, batch_size, num_entities, corrupt_size, num_relations, train_both)\n",
        "#         print(f\"Epoch {epoch} loss: {train_loss}\")\n",
        "\n",
        "#         if epoch % save_per_iter == 0:\n",
        "#             model.save(f\"{data_path}/model_epoch_{epoch}\")\n",
        "\n",
        "#         if epoch % save_per_iter == 0:\n",
        "#             print(f\"Evaluating on dev data at epoch {epoch}\")\n",
        "#             dev_accuracy = evaluate(model, indexed_dev_data, batch_size, num_entities, num_relations)\n",
        "#             print(f\"Dev accuracy at epoch {epoch}: {dev_accuracy}\")\n",
        "\n",
        "#     print(\"Training finished!\")\n",
        "#     print(\"Evaluating on test data\")\n",
        "#     test_accuracy = evaluate(model, indexed_test_data, batch_size, num_entities, num_relations)\n",
        "#     print(f\"Test accuracy: {test_accuracy}\")\n",
        "\n",
        "# run_training()\n",
        "def run_training():\n",
        "    print(\"Begin!\")\n",
        "    print(\"Load training data...\")\n",
        "    raw_training_data = load_data(data_path, training_string)\n",
        "    print(\"Load validation data...\")\n",
        "    raw_dev_data = load_data(data_path, dev_string)\n",
        "    print(\"Load test data...\")\n",
        "    raw_test_data = load_data(data_path, test_string)\n",
        "\n",
        "    print(\"Load entities and relations...\")\n",
        "    entities_list = load_entities(data_path)\n",
        "    relations_list = load_relations(data_path)\n",
        "    indexed_training_data = data_to_indexed(raw_training_data, entities_list, relations_list)\n",
        "    indexed_dev_data = data_to_indexed(raw_dev_data, entities_list, relations_list)\n",
        "    indexed_test_data = data_to_indexed(raw_test_data, entities_list, relations_list)\n",
        "\n",
        "    print(\"Load embeddings...\")\n",
        "    init_word_embeds, entity_to_wordvec = load_init_embeds(data_path)\n",
        "\n",
        "    num_entities = len(entities_list)\n",
        "    num_relations = len(relations_list)\n",
        "    num_iters = 10\n",
        "    batch_size = 64\n",
        "    corrupt_size = 10\n",
        "    slice_size = 3\n",
        "    regularization = 0.0001\n",
        "    learning_rate = 0.001\n",
        "    save_per_iter = 10\n",
        "    train_both = False\n",
        "\n",
        "    print(f\"Starting to build model {datetime.datetime.now()}\")\n",
        "\n",
        "    model = CustomModel(init_word_embeds, entity_to_wordvec, num_entities, num_relations, slice_size, batch_size)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    for epoch in range(1, num_iters + 1):\n",
        "        print(f\"Starting epoch {epoch} {datetime.datetime.now()}\")\n",
        "        train_loss = train_epoch(model, optimizer, loss_fn, indexed_training_data, batch_size, num_entities, corrupt_size, num_relations, train_both)\n",
        "        print(f\"Epoch {epoch} loss: {train_loss}\")\n",
        "\n",
        "        if epoch % save_per_iter == 0:\n",
        "            model.save(f\"{data_path}/model_epoch_{epoch}\")\n",
        "\n",
        "        if epoch % save_per_iter == 0:\n",
        "            print(f\"Evaluating on dev data at epoch {epoch}\")\n",
        "            dev_accuracy = evaluate(model, indexed_dev_data, batch_size, num_entities, num_relations)\n",
        "            print(f\"Dev accuracy at epoch {epoch}: {dev_accuracy}\")\n",
        "\n",
        "    print(\"Training finished!\")\n",
        "    print(\"Evaluating on test data\")\n",
        "    test_accuracy = evaluate(model, indexed_test_data, batch_size, num_entities, num_relations)\n",
        "    print(f\"Test accuracy: {test_accuracy}\")\n",
        "\n",
        "run_training()\n"
      ]
    }
  ]
}